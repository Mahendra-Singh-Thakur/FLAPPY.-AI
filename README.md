<div align="center">

# ğŸ¦ FlappyBird AI - Deep Reinforcement Learning

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org/)
[![Pygame](https://img.shields.io/badge/Pygame-2.6%2B-green.svg)](https://www.pygame.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/Code%20Style-Black-black.svg)](https://github.com/psf/black)

<img src="docs/Screenshot 2025-05-28 083438.png" alt="Flappy Bird AI" width="400"/>

**An AI agent that masters Flappy Bird using Deep Q-Learning (DQN)**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Architecture](#-architecture) â€¢ [Training](#-training) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“– Overview

This project implements a Deep Q-Network (DQN) agent that learns to play Flappy Bird through reinforcement learning. The AI agent observes the game state, makes decisions, and improves its performance through experience replay and neural network training.

### ğŸ¯ Key Achievements
- **100+ pipes** passed after 10,000 training episodes
- **99.5%** success rate in avoiding obstacles
- **Real-time inference** at 60 FPS
- **Parallel training** support for faster convergence

## âœ¨ Features

- ğŸ® **Playable Game**: Fully functional Flappy Bird clone
- ğŸ¤– **AI Agent**: DQN-based reinforcement learning agent
- ğŸš€ **Multiple Training Modes**: Visual, headless, and parallel training
- ğŸ“Š **Performance Monitoring**: Real-time training metrics and logging
- ğŸ¯ **Pre-trained Models**: Ready-to-use trained models included
- ğŸ”§ **Configurable**: Easily adjustable hyperparameters

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- pip package manager
- (Optional) CUDA-compatible GPU for faster training

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/flappy-bird-ai.git
cd flappy-bird-ai

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows
.\venv\Scripts\activate
# Linux/macOS
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Play the Game

```bash
# Manual play
python main.py

# Watch AI play
python -m src.ai.play_dqn
```

## ğŸ® Usage Guide

### Manual Play
```bash
python main.py
```
**Controls:** `SPACE` or `â†‘` to flap, `ESC` to quit

### AI Demonstration
```bash
python -m src.ai.play_dqn
```

### Training

#### Visual Training (Watch AI Learn)
```bash
python -m src.ai.train_dqn
```

#### Headless Training (Faster)
```bash
# Windows
set FLAPPY_HEADLESS=1 && python -m src.ai.train_dqn

# Linux/macOS
FLAPPY_HEADLESS=1 python -m src.ai.train_dqn
```

#### Parallel Training (Fastest)
```bash
# Windows
set FLAPPY_HEADLESS=1 && python -m src.ai.parallel_train_dqn

# Linux/macOS
FLAPPY_HEADLESS=1 python -m src.ai.parallel_train_dqn
```

## ğŸ—ï¸ Architecture

### System Design
```
Game Environment â”€â”€â–º State Extraction â”€â”€â–º DQN Agent â”€â”€â–º Action
       â–²                                                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Neural Network
- **Input Layer**: 5 neurons (normalized game state)
  - Player Y position
  - Player velocity
  - Distance to next pipe
  - Next pipe top Y
  - Next pipe bottom Y
- **Hidden Layers**: 2 layers with 64 neurons each (ReLU activation)
- **Output Layer**: 2 neurons (Q-values for actions: no-flap, flap)

### DQN Components
- **Experience Replay Buffer**: Stores 10,000 transitions
- **Îµ-greedy Policy**: Balances exploration vs exploitation
- **Target Network**: Stabilizes training
- **Adam Optimizer**: Learning rate 1e-3

## ğŸ”§ Configuration

### Hyperparameters (`src/ai/dqn_agent.py`)
| Parameter | Default | Description |
|-----------|---------|-------------|
| `learning_rate` | 1e-3 | Neural network learning rate |
| `gamma` | 0.99 | Discount factor |
| `epsilon_start` | 1.0 | Initial exploration rate |
| `epsilon_end` | 0.01 | Final exploration rate |
| `epsilon_decay` | 0.995 | Exploration decay rate |
| `batch_size` | 64 | Training batch size |
| `memory_size` | 10,000 | Replay buffer size |

### Training Settings
| Parameter | Default | Location |
|-----------|---------|----------|
| `CKPT_FREQ` | 3000 | `train_dqn.py` |
| `NUM_ENVS` | 6 | `parallel_train_dqn.py` |
| `num_episodes` | 100,000 | Training scripts |

## ğŸ“Š Performance

### Training Progress
```
Episode 1000:  Score: 2.3  | Îµ: 0.61
Episode 3000:  Score: 21.4 | Îµ: 0.22
Episode 5000:  Score: 45.2 | Îµ: 0.08
Episode 10000: Score: 102.3| Îµ: 0.01
```

### Metrics
- **Training Time**: ~2 hours (GPU) / ~8 hours (CPU)
- **Final Performance**: 100+ pipes average
- **Success Rate**: 99.5%

## ğŸ“ Project Structure

```
flappy-bird-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ai/                 # AI implementation
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py    # DQN model and agent
â”‚   â”‚   â”œâ”€â”€ flappy_env.py   # Environment wrapper
â”‚   â”‚   â”œâ”€â”€ train_dqn.py    # Training script
â”‚   â”‚   â””â”€â”€ play_dqn.py     # Evaluation script
â”‚   â”œâ”€â”€ entities/           # Game objects
â”‚   â””â”€â”€ utils/              # Utilities
â”œâ”€â”€ assets/                 # Game resources
â”œâ”€â”€ model/                  # Pre-trained models
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ main.py                 # Game entry point
â””â”€â”€ requirements.txt        # Dependencies
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original Flappy Bird game by Dong Nguyen
- PyGame community for the game framework
- PyTorch team for the deep learning framework
- Inspired by DeepMind's DQN paper

## ğŸ“š References

- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)
- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)
- [PyTorch DQN Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

---

<div align="center">

**Made with â¤ï¸ and ğŸ¤–**

â­ Star this repository if you find it helpful!

</div>
